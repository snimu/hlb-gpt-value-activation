# hlb-gpt-value-activation
Check out how much of a difference the activation of the value makes vs. keeping it linear as in standard attention
