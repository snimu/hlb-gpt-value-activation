# hlb-gpt-value-activation
Check out how much of a difference the activation of the value makes vs. keeping it linear as in standard attention

Based entirely on v0.4.0 of [hlb-gpt](https://github.com/tysam-code/hlb-gpt/tree/main).
If you want to cite anything, cite that: `@software{hlb-gpt_2024, author={Fern}, month={3}, title={{hlb-gpt}}, url={https://github.com/tysam-code/hlb-gpt}, version = {0.4.0}, year = {2024}}`.

This will only be a tiny experiment.
